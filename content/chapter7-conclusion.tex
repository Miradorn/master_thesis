% !TEX root = ../thesis.tex
%
\chapter{Conclusion}
\label{sec:conclusion}

\section{Conclusion for TesslaServer}

note limitations of Type system of compiler

\section{Conclusion for the Instrumentation Pass}

In the benchmarks of the instrumentation it is obvious that it adds a lot of overhead, especially when compiler optimizations are turned on or the percentage of instrumented function calls is large.
While the overhead is big, it is stable (see the standard deviation in \cref{sec:evaluation:instrumentation_benchmark:instr_vs_non_inst}).

For now the instrumentation should be seen as a proof of concept and test tool.
The generated traces should be mostly used for analysing test settings, where compiler optimizations are turned off.
When used for timing specifications, the instrumented code can be benchmarked against a non instrumented version of the code and the results can be used to transform the actual timing requirements to corresponding ones for the instrumented code.
This will at least give an approximation of the actual results.
Also it is recommended to write small and specific \gls{tessla} specifications, so that the instrumentations only have to be applied to a small subset of functions.
Obviously it is strongly discouraged to use the instrumented code in any production setting.

\section{Further Work}
\label{sec:conclusion:further_work}

Composition of Transducers/evalEngines
Port to Scala/akka
Different evaluation model: Pull not push
  port to genstage: Concept of backpressure -> Nodes generating events out of nothing


online monitoring -> possible infinite traces

Architecture is similar to a vm: Tessla specs are code, compiler produces intermediate representation (json), runtime executes Ir.
Therefore: Maybe define new functions (read nodes) in the spec itself and not in the runtime? Also recursion in specifications

\subsection{Error prevention}
\label{sec:conclusion:further_work:error_prevention}
Ways of sending observations back to the program to recover from or prevent errors

